{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libs\n",
    "# For example: \n",
    "# import os\n",
    "\n",
    "# from lib.agents import Agent\n",
    "# from lib.llm import LLM\n",
    "# from lib.messages import UserMessage, SystemMessage, ToolMessage, AIMessage\n",
    "# from lib.tooling import tool\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from lib.agents import Agent\n",
    "from lib.llm import LLM\n",
    "from lib.messages import BaseMessage\n",
    "from lib.tooling import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "CHROMA_OPENAI_API_KEY = os.getenv(\"CHROMA_OPENAI_API_KEY\") or OPENAI_API_KEY\n",
    "\n",
    "assert OPENAI_API_KEY is not None, \"Missing OPENAI_API_KEY in .env\"\n",
    "assert TAVILY_API_KEY is not None, \"Missing TAVILY_API_KEY in .env\"\n",
    "assert CHROMA_OPENAI_API_KEY is not None, \"Missing CHROMA_OPENAI_API_KEY (or OPENAI_API_KEY) in .env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce364221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create retrieve_game tool\n",
    "# It should use chroma client and collection you created\n",
    "# chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "# collection = chroma_client.get_collection(\"udaplay\")\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - query: a question about game industry. \n",
    "#\n",
    "#    You'll receive results as list. Each element contains:\n",
    "#    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "#    - Name: Name of the Game\n",
    "#    - YearOfRelease: Year when that game was released for that platform\n",
    "#    - Description: Additional details about the game\n",
    "\n",
    "CHROMA_DIR = Path(os.getenv(\"CHROMA_PATH\") or \"chromadb\").resolve()\n",
    "COLLECTION_NAME = \"udaplay\"\n",
    "MEMORY_COLLECTION_NAME = \"udaplay_long_term_memory\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "try:\n",
    "    embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=CHROMA_OPENAI_API_KEY,\n",
    "        model_name=\"text-embedding-3-small\",\n",
    "    )\n",
    "except TypeError:\n",
    "    embedding_fn = embedding_functions.OpenAIEmbeddingFunction(api_key=CHROMA_OPENAI_API_KEY)\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_fn,\n",
    " )\n",
    "memory_collection = chroma_client.get_or_create_collection(\n",
    "    name=MEMORY_COLLECTION_NAME,\n",
    "    embedding_function=embedding_fn,\n",
    " )\n",
    "\n",
    "@tool\n",
    "def retrieve_game(query: str, n_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Semantic search in the local game VectorDB.\"\"\"\n",
    "    res = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    docs = res.get(\"documents\", [[]])[0]\n",
    "    metas = res.get(\"metadatas\", [[]])[0]\n",
    "    dists = res.get(\"distances\", [[]])[0]\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for doc, meta, dist in zip(docs, metas, dists):\n",
    "        item = dict(meta or {})\n",
    "        item[\"_indexed_text\"] = doc\n",
    "        item[\"_distance\"] = dist\n",
    "        item[\"_source\"] = {\"type\": \"local_vector_db\", \"collection\": COLLECTION_NAME}\n",
    "        out.append(item)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create evaluate_retrieval tool\n",
    "# You might use an LLM as judge in this tool to evaluate the performance\n",
    "# You need to prompt that LLM with something like:\n",
    "# \"Your task is to evaluate if the documents are enough to respond the query. \"\n",
    "# \"Give a detailed explanation, so it's possible to take an action to accept it or not.\"\n",
    "# Use EvaluationReport to parse the result\n",
    "# Tool Docstring:\n",
    "#    Based on the user's question and on the list of retrieved documents, \n",
    "#    it will analyze the usability of the documents to respond to that question. \n",
    "#    args: \n",
    "#    - question: original question from user\n",
    "#    - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "#    The result includes:\n",
    "#    - useful: whether the documents are useful to answer the question\n",
    "#    - description: description about the evaluation result\n",
    "\n",
    "class EvaluationReport(BaseModel):\n",
    "    useful: bool = Field(description=\"Whether the documents are useful to answer the question\")\n",
    "    confidence: float = Field(description=\"Confidence score 0-1\", ge=0.0, le=1.0)\n",
    "    description: str = Field(description=\"Explanation for the decision\")\n",
    "    should_web_search: bool = Field(description=\"Whether to fall back to web search\")\n",
    "\n",
    "@tool\n",
    "def evaluate_retrieval(question: str, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate whether retrieved docs are sufficient; otherwise recommend web search.\"\"\"\n",
    "    judge = LLM(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a strict retrieval evaluator.\\n\"\n",
    "        \"Decide whether the retrieved documents are sufficient to answer the user's question accurately.\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        \"Retrieved documents (JSON):\\n\"\n",
    "        f\"{json.dumps(retrieved_docs, ensure_ascii=False)}\\n\\n\"\n",
    "        \"Return ONLY a JSON object matching this schema:\\n\"\n",
    "        \"{useful: bool, confidence: float (0-1), description: str, should_web_search: bool}\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- useful=true only if the docs directly contain the needed facts.\\n\"\n",
    "        \"- If the question is time-sensitive (e.g., 'right now/currently'), prefer web search.\\n\"\n",
    "        \"- should_web_search=true when useful is false OR confidence < 0.65.\\n\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        ai = judge.invoke(prompt, response_format=EvaluationReport)\n",
    "        report = EvaluationReport.model_validate_json(ai.content)\n",
    "    except Exception:\n",
    "        # Safe fallback heuristic if structured parsing fails\n",
    "        useful = bool(retrieved_docs)\n",
    "        report = EvaluationReport(\n",
    "            useful=useful,\n",
    "            confidence=0.5 if useful else 0.0,\n",
    "            description=\"Fallback evaluation (structured parsing failed).\",\n",
    "            should_web_search=not useful,\n",
    "        )\n",
    "\n",
    "    if (not report.useful) or (report.confidence < 0.65):\n",
    "        report.should_web_search = True\n",
    "    return report.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create game_web_search tool\n",
    "# Please use Tavily client to search the web\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - question: a question about game industry. \n",
    "\n",
    "@tool\n",
    "def game_web_search(question: str, search_depth: str = \"advanced\", max_results: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Search the web using Tavily API (fallback) and store a compact memory fragment.\"\"\"\n",
    "    client = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "    search_result = client.search(\n",
    "        query=question,\n",
    "        search_depth=search_depth,\n",
    "        include_answer=True,\n",
    "        include_raw_content=False,\n",
    "        include_images=False,\n",
    "        max_results=max_results,\n",
    "    )\n",
    "\n",
    "    formatted = {\n",
    "        \"answer\": search_result.get(\"answer\", \"\"),\n",
    "        \"results\": search_result.get(\"results\", []),\n",
    "        \"search_metadata\": {\"timestamp\": datetime.now().isoformat(), \"query\": question},\n",
    "        \"_source\": {\"type\": \"tavily\"},\n",
    "    }\n",
    "\n",
    "    # Persist a compact memory fragment for future use\n",
    "    urls = [r.get(\"url\") for r in formatted.get(\"results\", []) if isinstance(r, dict) and r.get(\"url\")]\n",
    "    memory_text = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Web answer: {formatted.get('answer','')}\\n\"\n",
    "        f\"URLs: {json.dumps(urls, ensure_ascii=False)}\"\n",
    "    )\n",
    "    metadata = {\n",
    "        \"kind\": \"web_search_result\",\n",
    "        \"question\": question,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"urls\": json.dumps(urls, ensure_ascii=False),\n",
    "    }\n",
    "    doc_id = f\"web_{uuid.uuid4().hex}\"\n",
    "    try:\n",
    "        if hasattr(memory_collection, \"upsert\"):\n",
    "            memory_collection.upsert(ids=[doc_id], documents=[memory_text], metadatas=[metadata])\n",
    "        else:\n",
    "            memory_collection.add(ids=[doc_id], documents=[memory_text], metadatas=[metadata])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your Agent abstraction using StateMachine\n",
    "# Equip with an appropriate model\n",
    "# Craft a good set of instructions \n",
    "# Plug all Tools you developed\n",
    "\n",
    "tools = [retrieve_game, evaluate_retrieval, game_web_search]\n",
    "\n",
    "INSTRUCTIONS = (\n",
    "    \"You are UdaPlay, an AI research agent for the video game industry.\\n\"\n",
    "    \"You have tools to retrieve from a local VectorDB and to search the web.\\n\\n\"\n",
    "    \"Workflow you MUST follow:\\n\"\n",
    "    \"1) Call retrieve_game first.\\n\"\n",
    "    \"2) Call evaluate_retrieval(question, retrieved_docs).\\n\"\n",
    "    \"3) If should_web_search is true, call game_web_search(question).\\n\\n\"\n",
    "    \"Final answer requirements:\\n\"\n",
    "    \"- Provide a concise response and include citations (URLs) if web search was used.\\n\"\n",
    "    \"- If you cannot find the answer, say you don't know. Do not fabricate.\\n\"\n",
    " )\n",
    "\n",
    "agent = Agent(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    tools=tools,\n",
    "    temperature=0.2,\n",
    " )\n",
    "\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Invoke your agent\n",
    "# - When Pokémon Gold and Silver was released?\n",
    "# - Which one was the first 3D platformer Mario game?\n",
    "# - Was Mortal Kombat X realeased for Playstation 5?\n",
    "\n",
    "def print_trace(messages: List[BaseMessage]):\n",
    "    for m in messages:\n",
    "        tool_calls = getattr(m, \"tool_calls\", None)\n",
    "        if tool_calls:\n",
    "            print(f\"-> role={m.role} tool_calls={[tc.function.name for tc in tool_calls]}\")\n",
    "        else:\n",
    "            print(f\"-> role={m.role} content={str(m.content)[:200]}\")\n",
    "\n",
    "queries = [\n",
    "    \"When was Pokémon Gold and Silver released?\",\n",
    "    \"Which one was the first 3D platformer Mario game?\",\n",
    "    \"Was Mortal Kombat X released for PlayStation 5?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"USER:\", q)\n",
    "    run = agent.invoke(q, session_id=\"demo\")\n",
    "    final_state = run.get_final_state()\n",
    "    messages = final_state.get(\"messages\", []) if final_state else []\n",
    "    print(\"\\nTRACE:\")\n",
    "    print_trace(messages)\n",
    "    if messages:\n",
    "        print(\"\\nFINAL ANSWER:\")\n",
    "        print(messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update your agent with long-term memory\n",
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes\n",
    "\n",
    "# Note: In this implementation, long-term memory is already persisted via the\n",
    "# `udaplay_long_term_memory` ChromaDB collection inside `game_web_search()`.\n",
    "# The `Agent` is already implemented using a StateMachine in lib/agents.py."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
